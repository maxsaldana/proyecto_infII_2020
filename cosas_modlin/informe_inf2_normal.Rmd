---
title: "Proyecto Inferencia II"
author: "Emanuelle Marsella, Maximiliano Saldaña, Lucia Tafernaberry"
date: "Noviembre 2020"
output:
  pdf_document:
    toc: no
    pandoc_args: [
      "--number-sections",
      "--number-offset=1"
    ]
header-includes:
  - \usepackage{float}
  - \usepackage[spanish]{babel}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,
                      include=FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = 'H',
                      fig.align = 'center',
                      out.extra = '',
                      fig.hold = 'hold',
                      out.width = "50%"
                      )
options(xtable.comment=FALSE,
        xtable.table.placement="H")
```


```{r, include = FALSE}
library(dplyr)
library(readr)
library(forcats)
library(ggplot2)
library(janitor)

library(xtable)
library(scales)

library(rstanarm)
library(bayesplot)



```

\title{Modelización lineal del salario} 
 \author{Emanuelle Marsella, Maximiliano Saldaña, Lucia Tafernaberry} 
 \date {Noviembre 2020} 



\maketitle

<!-- \vspace{5cm} -->
<!-- ![](unnamed.jpg){width = 50%} -->

\newpage

\tableofcontents

\newpage

# Introducción

En este proyecto aplicaremos los conceptos y métodos bayesianos aprendidos en el curso Inferencia II para analizar los datos seleccionados, una base de datos sobre salarios del año 1976 en Estados Unidos extraída del libro _Introducción a la Econometría, Un enfoque moderno (2009)_ de J.M. Wooldridge. Nuestra intención es tratar de explicar la variable principal, el salario por hora medido en dólares, a partir de las otras variables de la base. Para lograr esto haremos uso de modelos de regresión lineal y metodologías bayesianas asociadas, que implementaremos mediante el software R.   


# Descripción de los datos

La base cuenta con observaciones de 526 personas y con las siguientes 22 variables: Salario (promedio por hora, medido en dólares), Años de Educación, Años de Experiencia, Antigüedad, Raza (variable indicadora, 1 si la persona no es de raza blanca), Sexo, Estado Civil (vale 1 si la persona está casada), Número de Dependientes, Región Metropolitana (vale 1 si la persona vive en dicha región), Región (dividida en tres variales indicadoras: Norte, Sur y Oeste), Rama de Actividad (dividida en tres variables indicadoras: Construcción, Comercio y Servicios), Ocupación (tres variables indicadoras: Profesional, Administrativos y Servicios), el logaritmo de la variable Salario y los cuadrados de las variables Experiencia y Antigüedad.

```{r, results=FALSE, echo=FALSE}
datos <- read_delim("wage1.txt", delim = "\t") %>% 
  as.data.frame %>% 
  clean_names() %>%
  rename(salario = "wage", reg.metro = "smsa", log.salario= "lwage")

as.numeric(sapply(datos, is.numeric))

names(subset(datos, select=!as.numeric(sapply(datos, is.numeric))))

sapply(subset(datos, select=!as.numeric(sapply(datos, is.numeric))), class)

head(subset(datos, select=!as.numeric(sapply(datos, is.numeric))))

datos <- datos %>%
  mutate(salario = gsub(",", ".", salario), log.salario = gsub(",", ".", log.salario) ) %>% 
  mutate_at(c("salario", "log.salario"), as.numeric)
```


## Variables Indicadoras
Inicialmente modificamos la base para llevarla a una forma que nos parecía más práctica de trabajar, agrupando variables que refieren a una sola característica de los datos y estaban en una forma binaria. Estas variables son la Región, separada en la base original en Norte, Sur y Oeste; la Rama de Actividad, separada en Construcción, Servicios y Comercio; y la Ocupación, separada en Profesional, Administrativos y Servicios. Luego de consultar a las docentes de Modelos Lineales optamos por no descartar las variables indicadoras ya que nos permiten su utilización a la hora de aplicar un modelo lineal multivariado y a su vez mantenemos la variable agrupada para poder usarla a la hora de manipular y resumir datos.

```{r, results=FALSE, echo=FALSE}
#mod datos 
datos <- datos %>% 
  mutate(
    region = as.factor(case_when(
               northcen == 1 ~ "northcen",
               south == 1 ~ "south",
               west == 1 ~ "west",
               northcen + south + west == 0 ~ "east"
               )),
    rama_act = as.factor(case_when(
                 construc == 1 ~ "construc",
                 trade == 1 ~ "trade",
                 services == 1 ~ "services",
                 construc + trade + services == 0 ~ "otros"
               )),
    ocupacion = as.factor(case_when(
                 profocc == 1 ~ "profocc",
                 clerocc == 1 ~ "clerocc",
                 servocc == 1 ~ "servocc",
                 profocc + clerocc + servocc == 0 ~ "otros"
                )),
    educsq = educ^2
  ) %>% 
  dplyr::select(-profserv)

#%>% 
 # select(-c(northcen,south, west,construc,trade,services,profserv,profocc,clerocc,servocc))

```

# Exploracion inicial

Para familiarizarnos con la base utilizamos medidas de resumen. 

## Variables categóricas

Lo primero que hacemos es visualizar cómo se distribuye la población de acuerdo a las variables categóricas de las que disponemos (Región, Rama de actividad,  Ocupación, Raza, Sexo y Estado civil). En cuanto a las primeras cuatro variables buscamos saber cuántos individuos pertenecen a cada rama, así como aplicar algunas medidas de resumen de la variable salario para cada categoría de las tres variables, como se puede ver en los cuadros a continuación.


```{r tablasregion, results='asis'}
resumen_tot1 <- datos %>%
                 summarise(region = "Todas",
                           Cantidad=n(),
                           `Mínimo`= min(salario),
                           Media=mean(salario),
                           `Máximo` = max(salario)) %>% 
                rename(`Región` = "region")

resumen_tot2 <- datos %>%
                 summarise(rama_act = "Todas",
                           Cantidad=n(),
                           `Mínimo`= min(salario),
                           Media=mean(salario),
                           `Máximo` = max(salario)) %>% 
                 rename(`Rama de Actividad` = "rama_act")

resumen_tot3 <- datos %>%
                 summarise(ocupacion = "Todas",
                           Cantidad=n(),
                           `Mínimo`= min(salario),
                           Media=mean(salario),
                           `Máximo` = max(salario)) %>%
                 rename(`Ocupación` = "ocupacion")
                 
resumen_reg <- datos %>%
                group_by(region) %>%
                summarise(Cantidad=n(),
                          `Mínimo`= min(salario),
                          Media=mean(salario),
                          `Máximo` = max(salario)) %>%
                arrange(fct_relevel(region,
                                    "northcen",
                                    "south",
                                    "east",
                                    "west")) %>% 
                mutate(region = fct_recode(region,
                                           "Norte"="northcen",
                                           "Sur"="south",
                                           "Este"="east",
                                           "Oeste"="west")) %>% 
                rename(`Región`="region")
  
resumen_rama_act <- datos %>% 
                     group_by(rama_act) %>% 
                     summarise(Cantidad=n(),
                               `Mínimo`= min(salario),
                               Media=mean(salario),
                               `Máximo` = max(salario)) %>%
                     arrange(fct_relevel(rama_act,
                                         "construc",
                                         "services",
                                         "trade",
                                         "otros")) %>% 
                     mutate(rama_act = fct_recode(rama_act,
                                                  "Construcción"="construc",
                                                  "Servicios"="services",
                                                  "Comercio"="trade",
                                                  "Otros"="otros")) %>% 
                     rename(`Rama de Actividad` = "rama_act") 

resumen_ocupacion <- datos %>% 
                      group_by(ocupacion) %>% 
                      summarise(Cantidad=n(),
                                `Mínimo`= min(salario),
                                Media=mean(salario),
                                `Máximo` = max(salario)) %>%
                      arrange(fct_relevel(ocupacion,
                                          "servocc",
                                          "clerocc",
                                          "profocc",
                                          "otros")) %>% 
                      mutate(ocupacion = fct_recode(ocupacion,
                                                   "Servicio"="servocc",
                                                   "Administrativos"="clerocc",
                                                   "Profesional"="profocc",
                                                   "Otros"="otros")) %>%
                      rename(`Ocupación` = "ocupacion")
                     


  


propfem <- table(as.factor(datos$female))/dim(datos)[1]
propwhite <- table(as.factor(datos$nonwhite))/dim(datos)[1]
propmarr <- table(as.factor(datos$married))/dim(datos)[1]
propmetro <- table(as.factor(datos$reg.metro))/dim(datos)[1]
```

### Región
La variable región nos indica en qué región se ubican los individuos. Las categorías de esta variable son Norte, Sur, Este y Oeste.

Tenemos 3 variables indicadoras para esta región, Norte-Centro, Sur y Oeste. Este es la categoría de referencia, la cual no cuenta con variable indicadora ya que en este caso la matriz de datos $\mathbb{X}$ no sería de rango completo y por lo tanto no sería invertible, lo que más adelante impediría la estimación única de los coeficientes. Esto ocurre análogamente para las otras variables cualitativas.

```{r muestrotablasregion, results='asis', include=TRUE}
rbind(resumen_reg, resumen_tot1) %>% 
       xtable(caption = "Cantidad de observaciones, media, mínimo y máximo del salario para cada región.")
```

En primer lugar, podemos observar que la región Oeste cuenta con un número considerablemente pequeño de observaciones y la región sur cuenta con muchas observaciones, en comparación con las demás.

En cuanto al salario por hora, vemos que las medias para las regiones Norte y Sur son menores a la media del total de observaciones, mientras que para las regiones Este y Oeste son mayores. Por otro lado, los máximos son similares, si bien apreciamos que el máximo total se observa en la región este.

Los mínimos también son similares para todas las regiones excepto para la región Oeste, para la cual podemos apreciar que el mínimo es casi 3 veces menor que para las demás.

Como parte de la exploración inicial, también realizamos algunos gráficos de dispersión del salario en función de algunas variables cuantitativas. En ellos podemos apreciar que no hay demasiada diferencia entre las regiones a la hora de explicar el salario a partir de las distintas variables explicativas de forma individual, por lo cual optamos por no incluirlos en este documento.


### Rama de actividad
```{r muestrotablasramas_act, results='asis', include=TRUE}
 rbind(resumen_rama_act, resumen_tot2) %>% 
       xtable(caption = "Cantidad de observaciones, media, mínimo y máximo del salario para cada rama de actividad.")
```

Lo primero que observamos, es que la cantidad de observaciones para las distintas categorías varía mucho. En particular, para la categoría "Otros", que es la de referencia, tenemos un gran número de individuos, más de la mitad. Consideramos que esto se debe a que la diferenciación de la que disponemos para las diferentes ramas de actividad no es exhaustiva (como sí sucedía para las regiones, que solo son 4). Por lo tanto, es lógico que suceda que muchos individuos no pertenezcan ni al sector de Construcción, ni al de Servicios, ni al de Comercio; sino que pertenecen a alguna otra categoría que no está especificada y queda incluída dentro de "Otros".

Comparando las regiones en media, observamos que la media de la categoría de referencia es mayor que en las demás ramas de actividad. La categoría Servicios tiene la menor media, y en esta categoría se observa el mínimo total, y el menor de los máximos. La categoría Comercio es similar en media a Servicios, si bien tanto su mínimo como su máximo son mayores. Finalmente, la categoría Construcción tiene la mayor media de las 3 indicadoras de las que disponemos, y también el menor mínimo.

### Ocupación
```{r muestrotablasocupacion, results='asis', include=TRUE}
rbind(resumen_ocupacion, resumen_tot3) %>% 
       xtable(caption = "Cantidad de observaciones, media, mínimo y máximo del salario para cada ocupación.")
```

Podemos observar que para la variable Ocupación, la media de la categoría Profesional es considerablemente mayor respecto de las demás, así como también su máximo. Es razonable esperar que así sea, que las personas que se desempeñan en un empleo profesional perciban un mayor salario, si bien no tenemos fundamentos para afirmar que esta diferencia sea estadísticamente significativa, dado que aún no hemos realizado inferencia a partir de nuestros datos. 

Por otro lado, la categoría Administrativos tiene el mayor mínimo y el menor máximo, y una media menor que la de la categoría de referencia, lo que sugiere que los niveles de salario no varían demasiado, y en general son menores que los de la categoría de referencia.

Luego, la categoría Servicio tiene el menor mínimo, la menor media, y el menor máximo, lo que sugiere que los individuos que se encuentran en esta categoría perciben en promedio un menor salario.


### Otras variables categóricas

Con el resto de las variables econtramos que las proporciones entre las categorías son:

* Para la variable Sexo hay un `r percent(propfem[2], accuracy = 0.01)` de mujeres y  `r percent(propfem[1], accuracy = 0.01)` de hombres. La proporción de la mitad cada sexo.

* Para la variable Estado Civil hay un `r percent(propmarr[2], accuracy = 0.01)` de casados y  `r percent(propmarr[1], accuracy = 0.01)` de no casados.

* Para la variable Área Metropolitana hay un `r percent(propmetro[2], accuracy = 0.01)` de personas que habitan en la misma y  `r percent(propmetro[1], accuracy = 0.01)` que no. Predomina bastante la población metropolitana.

* Para la variable Raza hay un `r percent(propwhite[2],accuracy = 0.01)` de personas no blancas  y  `r percent(propwhite[1], accuracy = 0.01)` blancas. El resultado es de esperarse dado que las personas no blancas son una minoría en los Estados Unidos.

# Métodos

## Presentación del modelo


Vamos a plantear un modelo de la forma
$$log(Y) =  X\beta + \varepsilon$$
donde hacemos los supuestos clásicos sobre los errores: normalidad, esperanza nula, homocedasticidad e incorrelación entre los mismos.

Nuestra variable explicada es el logaritmo del salario, que se puede interpretar de la siguiente forma: dado un aumento unitario en una de las variables explicativas $x_i$, la variación porcentual del salario equivale aproximadamente a $100 \beta_i$, a niveles constantes de todas las demás variables [^1], es decir:

[^1]: Introducción a la econometría: Un enfoque moderno. Wooldrige, Jeffrey. (2009). pág. 43.

$$\% \Delta y \simeq (100 \beta_i) \Delta x$$
$X$ es nuestra matriz de datos, que contiene los valores de las siguientes variables:
Experiencia, Antigüedad, Sexo, Reg. metropolitana, Norte/Sur, Comercio/servicios, Ocup. profesional, Exper. al cuadrado, Educ. al cuadrado, Indic. Obs. 128, Indic. Obs. 381 e Indic. Obs. 440.

Hubo un trabajo previo en el curso de Modelos Lineales para llegar a esta forma del modelo. La selección de las variables se hizo en base al criterio de información de Akaike y contrastes de significación conjunta, que sirvieron para agrupar las variables Norte con Sur y Comercio con Servicios. Además, para que se cumplieran los supuestos se tuvo que realizar una transformación de Box-Cox, excluirse una observación influyente de la base e incluirse un conjunto de variables indicadoras para representar un posible cambio en media y/o varianza en varias observaciones. Estas últimas variables indicadoras son de la forma:

$$z_t = \left\{
    \begin{array}{cc}
         & 1 \; \text{si} \; t=i     \\
         & 0 \; \text{si} \; t\neq i
    \end{array}
\right. $$

Lo que nos dejó con un modelo de la forma.

 $$log(y_t) =  x_t'\beta + \Delta_iz_t + \varepsilon_t$$


## Estimación del modelo 

La estimación de los parámetros del modelo se realizará desde un enfoque bayesiano, es decir, se les asignará una distribución previa para cuantificar la incertidumbre que se tiene sobre sus valores y utilizando los datos se obtendrá una distribución posterior. Se diferencia del caso de un modelo lineal clásico dado que en este último se trabaja con la distribución en el muestreo de lo estimadores de los parámetros y no con la distribución de los parámetros en si.

Realizamos la estimación mediante simulaciones Monte Carlo basadas en cadenas de Markov (MCMC), simulaciones en las cuales cada resultado depende únicamente del anterior. Para la estimación de cada uno de los parámetros se plantean 4 cadenas iniciadas en valores distintos, cada una con 4000 iteraciones. A partir de estos valores se avanza iterativamente hasta  llegar a los 4000 valores simulados, de los cuales los primeros 2000 se usan como "calentamiento" para desligar las simulaciones de los valores iniciales. 

Se desea que las cadenas converjan a la distribución posterior de los parámetros, y aunque nunca podemos asegurar que se da al ser un concepto teórico podemos evaluar si hay indicios de que no se da. Dos métodos para evaluar la convergencia  son el $\hat{R}$ y el $\hat{n}_{eff}$. 

 $$\hat{R} = \sqrt{\frac{Var^+(\theta | y)}{W}} $$
 
Donde:

$$Var^+(\theta | y) = \frac{n-1}{n}W + \frac{1}{n} B $$

En la expresión anterior n indica el número de iteraciones en cada cadena, W la media de las distintas varianzas dentro de las cadenas y B la varianza entre las medias de las cadenas. Si el $\hat{R}$ se acerca a uno, eso significa que la variabilidad entre las cadenas es pequeña y por lo tanto la variabilidad de las simulaciones es explicada por la variabilidad dentro de las cadenas.

Por otro lado definimos el indicador:

$$n_{\text{eff}} = \frac{n.m}{1 + 2 \sum^{\infty}_{k = 1} \rho_k}$$
Donde $n$ es el número de iteraciones, $m$ es el número de cadenas y $\rho_k$ la autocorrelación $k-ésima$. Al estimar $\sum^{\infty}_{k = 1} \rho_k$ conseguimos $\hat{n}_{\text{eff}}$. Este indicador es una estimación de la cantidad de muestras independientes equivalentes a las del método MCMC. 


# Resultados

```{r datosmod, results=FALSE, echo=FALSE }
datos2 <- dplyr::select(datos, -c(region, rama_act, ocupacion)) 



dummy2 <- c(rep(0, 1, 526))
dummy2[128] <- 1
datos2$i128 <- dummy2

dummy4 <- c(rep(0, 1, 526))
dummy4[381] <- 1
datos2$i381 <- dummy4



dummy3 <- c(rep(0, 1, 526))
dummy3[440] <- 1
datos2$i440 <- dummy3

datos2 <- datos2[-24,]

datos2 <- datos2 %>% 
  mutate(
    norsur = ifelse(northcen == 1 | south == 1, 1, 0), 
    comserv = ifelse(trade == 1 | services == 1, 1, 0)
    )

```

El modelo a estimar es:
$$log(salario) = \beta_0 + \beta_1exper + \beta_2tenure + \beta_3female + \beta_4reg.metro + \beta_5norsur + \beta_6comserv + \beta_7profocc$$
$$+ \beta_8expersq + \beta_9educsq  + \beta_{10} z_{128} + \beta_{11} z_{381}+ \beta_{12} z_{440} + u_i$$

Mantenemos la familia de distribuciones normales para la distribución de los errores que viene por defecto en la función _stan_glm()_ y consecuentemente las previas de los parámetros son definidas como $\beta_0 \sim Normal(1,6 ; 2,5)$ y $\beta_j \sim Normal(0 ; 2,5) \,\, \forall j = 1,...,12$. La media de $\beta_0$ es la media muestral del logaritmo de los salarios, nuestra variable dependiente. A partir de eso, los valores de los desvíos son ajustados por la función de la siguiente forma:

$$\beta_0 \sim Normal(1,6; 2,5\cdot s_y)$$

$$\beta_j \sim Normal(0; 2,5\cdot s_y/s_{x_j}) \,\, \forall j = 1, \dots ,12$$

Donde $s_y$ es el desvío del logaritmo del salario y $s_{x_j}$ es el desvío de la j-ésima variable explicativa. [^2]

Adicionalmente se consideró utilizar como distribución de los errores a la familia Gamma, que presenta un mejor ajuste en los datos replicados, como se puede ver en la siguiente figura.  


```{r, echo=FALSE}
mod_bayes0 <- stan_glm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440,
                      data = datos2,
                      family = gaussian(link = "identity"),
                      seed = 12345,
                      iter = 4000) 



mod_bayes <- stan_glm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440,
                      data = datos2,
                      family = Gamma(link = "log"), #ver que es esto
                      seed = 12345,
                      iter = 4000)

# mod_bayes2 <- stan_glm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440,
#                       data = datos2,
#                       family = Gamma(link = "identity"),
#                       seed = 12345,
#                       iter = 4000)

```

```{r, include = TRUE, fig.cap="Gráficos de las distribuciones de los valores simulados para las dos familias de errores consideradas.", fig.asp=0.25, out.width="200%"}
 gridExtra::grid.arrange(pp_check(mod_bayes0, nrep = 100) + ggtitle("Familia Normal"),
                         pp_check(mod_bayes, nrep = 100) + ggtitle("Familia Gamma"),
                         ncol = 2
                         )
```

En el modelo seleccionado encontramos el problema de que al comparar las $y$ replicadas con las originales el ajuste no es del todo bueno debido a que los datos simulados son simétricos a diferencia de los datos originales. Atribuimos esto a que elegimos una distribución Normal para los errores y por lo tanto también para los datos, cambiando la familia de distribuciones de los errores por la de las distribuciones Gamma, permite captar la asimetría de la variable explicada. Cabe destacar que en el caso de la familia Gamma como los valores del recorrido son positivos no se están captando aquellos valores del logaritmo del salario que resultan negativos, lo que sería una limitación del modelo. 

Otra limitación de ambos modelos es su incapacidad de captar la forma exacta de la distribución en la zona de mayor densidad, que cuenta con dos máximos locales, ocurriendo una subestimación en el primer máximo local y una sobreestimación en el segundo. Esto es más pronunciado en el modelo elegido, el de errores Normales. La utilización de un modelo de los errores que sea una mixtura de dos distribuciones Gamma es una posible opción para enfrentarse a este problema, si bien no la desarrollamos en este proyecto.   

A pesar del mejor ajuste del modelo con errores Gamma, la falta de claridad del funcionamiento del modelo y su interpretación, sumado a la evaluación de ambos mediante estadísticos nos llevó a descartar esta opción y optar por el modelo de errores Normales.

[^2]: [Documentación del paquete _rstanarm_.](http://mc-stan.org/rstanarm/articles/priors.html)



```{r modelo bayes y clasico, include=FALSE}
mod_clasico <- lm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440, data=datos2)

parametros.clasicos <- summary(mod_clasico)$coefficients[,1]

summary.mod <- as.data.frame(mod_bayes0$stanfit)

parametros.bayesianos <- mod_bayes0$coefficients

intervalos.posteriores <- posterior_interval(mod_bayes0, prob=0.95)

notacion.numerica <- function(x) {format(x, scientific=FALSE)}
redondear.6 <- function(x) {(round(as.numeric(x), 6))}
```


## Análisis de convergencia

En primer lugar, nos valdremos de distintos indicadores y visualizaciones que nos permitirán disgnósticar si existe o no falta de convergencia en nuestro modelo.

```{r graficos cadenas, include=TRUE, fig.cap="Gráficos que muestran la evolución de las cadenas para dos variables explicativas, el intercepto y la experiencia.", fig.asp=0.25, out.width="200%"}
gridExtra::grid.arrange(
  mcmc_trace(mod_bayes0,pars="(Intercept)"),
  mcmc_trace(mod_bayes0,pars="exper"),
  ncol=2
  )
```

Para observar el comportamiento de las cadenas usadas en la estimación, comenzamos con gráficos que nos permiten observar la evolución de las mismas a lo largo de las iteraciones. En los mismos podemos ver que las cadenas para los parámetros del intercepto y de la experiencia se mantienen en torno a un cierto valor en cada caso, lo cual sugiere que no hay falta de convergencia. El comportamiento para los demás parámetros es similar, presentándose estos dos casos en el documento a modo de ejemplo.


Además de las visualizaciones, nos valemos de dos medidas que nos permiten identificar la falta de convergencia o indicios de lo contrario: el $\hat R$ y el $\hat{n}_{\text{eff}}$.

```{r rhat, include=FALSE}
rhat <- summary(mod_bayes0) %>% as.data.frame() %>% select(Rhat)
neff <- summary(mod_bayes0) %>% as.data.frame() %>% select(n_eff)

print.xtable(
  cbind("Parámetro"=c("Intercepto", 'Experiencia', 'Antigüedad', 'Sexo', 'Región Metropolitana',
                      'Norte/Sur', 'Comercio/Servicios', 'Ocupación Profesional', 'Exp. al cuadrado',
                      'Educ. al cuadrado', 'Indic. 128', 'Indic. 381', 'Indic. 440'),
        "Rhat"=redondear.6(rhat[1:13,]),
        "neff" = redondear.6(neff[1:13,])
        ) %>% xtable(caption="Indicadores sobre convergencia para los distintos parámetros del modelo bayesiano."),
  include.rownames=FALSE)
```

\begin{table}[H]
\centering
\begin{tabular}{lll}
  \hline
Parámetro & $\hat{R}$ & $n_{\text{eff}}$ \\ 
  \hline
Intercepto & 0.999962 & 8743 \\ 
  Experiencia & 0.999883 & 6048 \\ 
  Antigüedad & 0.999631 & 9652 \\ 
  Sexo & 0.999988 & 10735 \\ 
  Región Metropolitana & 1.000015 & 10467 \\ 
  Norte/Sur & 0.999736 & 10541 \\ 
  Comercio/Servicios & 0.999939 & 10101 \\ 
  Ocupación Profesional & 0.999729 & 8976 \\ 
  Exp. al cuadrado & 0.999974 & 6128 \\ 
  Educ. al cuadrado & 1.000216 & 8528 \\ 
  Indic. 128 & 0.999891 & 10472 \\ 
  Indic. 381 & 0.999971 & 11287 \\ 
  Indic. 440 & 0.999742 & 10776 \\ 
   \hline
\end{tabular}
\caption{Indicadores sobre convergencia para los distintos parámetros del modelo bayesiano.} 
\end{table}


En este cuadro, podemos ver que para todos los parámetos del modelo bayesiano (del intercepto y las variables explicativas) el valor del $\hat{R}$ está en torno a 1 pero sin superar 1.1, lo cual sirve como regla empírica para desestimar indicios de falta de convergencia. En cuanto al $n_\text{eff}$, los valores son altos en comparación con las 4000 iteraciones dependientes realizadas por cadena, entre 6000 y 12000 simulaciones independientes.

## Distribuciones posteriores y medidas de resumen

```{r cuadro parametros, include=TRUE, results='asis', message=FALSE}
print.xtable(cbind("Parámetro"=c("Intercepto", 'Experiencia', 'Antigüedad', 'Sexo', 'Región Metropolitana', 'Norte/Sur', 'Comercio/Servicios', 'Ocupación Profesional', 'Exp. al cuadrado', 'Educ. al cuadrado', 'Indic. 128', 'Indic. 381', 'Indic. 440'),
      "Parámetros clásicos"= parametros.clasicos,
      "Parámetros bayesianos"=parametros.bayesianos[1:13],
      "Cuantil 2.5%"=round(intervalos.posteriores[1:13,1], 4),
      "Cuantil 97.5%"=round(intervalos.posteriores[1:13,2], 4)
      ) %>%
  as.data.frame() %>%
  mutate_at(vars(2,3), redondear.6) %>%
  xtable(digits = 6,
         caption="Cuadro comparativo entre las estimaciones de los parámetros realizados a través de inferencia clásica y bayesiana."), include.rownames = FALSE)

```

En el cuadro anterior podemos observar los resultados de la estimación de los coeficientes de las variables explicativas, tanto del modelo clásico por medio de mínimos cuadrados ordinarios, como del modelo bayesiano a través de estimación MCMC. Podemos ver que la diferencia es mínima entre ambas estimaciones, siendo los resultados iguales hasta al menos el segundo dígito después de la coma en todos los casos. Además, planteamos para cada parámetro el intervalo de credibilidad al 95%, y en todos los casos vemos que los parámetros estimados mediante inferencia clásica están comprendidos en dicho intervalo.

A continuación se presenta una representación gráfica que se corresponde con la información que se presenta en la tabla anterior, en donde los puntos representan la media de la distribución de cada parámetro y la linea el intervalo de credibilidad al 95%.

```{r, include=TRUE, fig.cap="Intervalos de credibilidad al 95\\% y media de la distribución cada parámetro.", fig.asp=0.5, out.width="200%"}
gridExtra::grid.arrange(
  mcmc_intervals(mod_bayes0, pars = names(parametros.bayesianos[1:10]), point_est = "mean", prob = 0,
  prob_outer = 0.95) + coord_flip() + theme(axis.text.x = element_text(angle = 45, vjust = 0.75)) ,
  mcmc_intervals(mod_bayes0, pars = names(parametros.bayesianos[11:13]), point_est = "mean", prob =0,
  prob_outer = 0.95) + coord_flip() + theme(axis.text.x = element_text(angle = 45)),
  ncol = 2
)
```


## Réplicas de los datos simulados

A continuación, veremos la capacidad predictiva de nuestro modelo, a través de la comparación de estadísticos aplicados tanto a nuestros datos originales como a una muestra de datos simulados.


```{r comparacion datos originales simulados, include=TRUE, fig.cap="Comparación de la media entre los datos originales y los simulados.", fig.asp=0.25, out.width="200%"}
pred<-posterior_predict(mod_bayes0,draws = 100)

pp_check(mod_bayes0, plotfun = "stat", stat = "mean") 
```

En el gráfico anterior se puede observar que la media de los datos originales, representada con una línea azul oscura, se ubica sobre los valores centrales del histograma, donde se dan los intervalos con mayor frecuencia relativa. Esto quiere decir que nuestros datos simulados son similares en media a nuestros datos originales, lo que era de esperarse ya que los modelos generalmente ajustan bien respecto a los estadísticos suficientes. Por este motivo no es un buen candidato para evaluar el desempeño del modelo.

```{r, include = TRUE, fig.cap="Comparación de la mediana entre los datos originales y los simulados.", fig.asp=0.25, out.width="200%" }
gridExtra::grid.arrange(pp_check(mod_bayes0, plotfun = "stat", stat = "median") + ggtitle("Familia Normal"),
                        pp_check(mod_bayes, plotfun = "stat", stat = "median") + ggtitle("Familia Gamma"),
                        ncol=2)
```

Se puede apreciar que para la distribución Normal de los datos el modelo no logra simular el valor de la mediana adecuadamente a diferencia del que utiliza la distribución Gamma de los errores que considerábamos como posible alternativa. Esto resulta un inconveniente que limita la capacidad explicativa del modelo. Continuamos la evaluación considerando el rango intercuartílico:

```{r, include = TRUE, fig.cap="Comparación del rango intercuartílico  entre los datos originales y los simulados.", fig.asp=0.25, out.width="200%" }
gridExtra::grid.arrange(pp_check(mod_bayes0, plotfun = "stat", stat = "IQR"),
                        pp_check(mod_bayes, plotfun = "stat", stat = "IQR"),
                        ncol=2)
```

Ahora tomando como estadístico de referencia el rango intercuartílico, observamos que el modelo elegido lo reproduce mejor. Si tomamos en cuenta este criterio nos inclinamos por el modelo con errores normales, si consideramos el de la replica de la mediana nos inclinaríamos por el que cuenta con errores de distribución Gamma, no obstante este último no cuenta con la claridad explicativa del primero como antes mencionábamos.   

## Interpretación de los coeficientes


```{r}

pars.bx100 <- parametros.bayesianos*100

nombres <- names(parametros.bayesianos)
xtable(as.data.frame(Nombres = nombres, Parsb = unname(parametros.bayesianos), parsb100 = unname(pars.bx100)))

cbind("Variable" = nombres,  "Valor del parámetro"=unname(parametros.bayesianos), "Valor por 100"=pars.bx100)  %>%  as.data.frame() %>% pivot_wider() %>% xtable(digits = 5) 

```

En el Cuadro **xx** presentamos los coeficientes estimados del modelo y los mismos multiplicados por 100. En primer lugar, si analizáramos el coeficiente de la experiencia por si solo, el aumento en la experiencia en un año manteniéndose todas las demás variables constantes se traduce en un aumento esperado del 3,08%. Sin embargo, notemos que también está presente la experiencia al cuadrado, por lo cual la variación porcentual del salario esperada dado un aumento unitario en la experiencia depende de los dos coeficientes. No es posible aislar el efecto de una u otra variable. Notemos también que el signo del coeficiente estimado para la experiencia al cuadrado es negativo, lo cual se interpreta como que a medida que aumenta la experiencia el crecimiento porcentual esperado en el salario será decreciente.  

En el caso de la antigüedad, un aumento unitario de la variable, con las demás constantes se ve reflejado en un aumento porcentual del 1.38% del salario esperado.

Por su parte un aumento en los años de educación tiene una relación cuadrática con el aumento porcentual en el salario (solo se encuentra presente la educación al cuadrado). Así, cuanto mayor sea la cantidad de años de educación el incremento porcentual del salario se irá haciendo mayor a su vez, por el contrario de como sucedió con la experiencia. De esta manera, considerando un individuo con $\mu$ años de educación un aumento unitario en esta variable supone un aumento porcentual del salario de $0,22 \times \Big((\mu+1)^2-\mu^2 \Big)$, dadas todas las demás variables constantes.  

 
**Variables cualitativas**
    
Cuando nos enfrentamos al analisis de las variables cualitativas, buscamos explicar la diferencia en la variación porcentual del salario al pertenecer a una categoría u otra de la variable, respecto de la de referencia. Lo primero que observamos es que se puede hacer una distinción en los ingresos esperados dependiendo del sexo de la persona analizada. Así, una mujer tendrá un salario esperado 27.7% menor que el de un hombre, aunque ambos tengan el mismo nivel educativo, experiencia, antigüedad, trabajen en la misma área y en definitiva sean indistinguibles para todas las variables a excepción de su sexo.  

Por otro lado, se aprecia una distinción entre las regiones metropolitana y no metropolitana, esperandose un salario 15.8% mayor en el primer caso, a iguales niveles de las demás variables.

Analizando la influencia de la ubicación geográfica en el salario esperado, para los individuos que se encuentran en la región Norte o en la Sur se espera que perciban un salario 8.79% menor que aquellos que se encuentran en las regiones Este u Oeste (las de referencia). Esto se corresponde con la estadística descriptiva realizada en la sección _Descripción de los datos_ (Cuadro 1), en la que observábamos que el salario es similar en cuanto a su media para las regiones Norte y Sur, la cual es superior a la de las regiones Este y Oeste.

En cuanto a las ramas de actividad observamos que las personas que pertenecen a las ramas Comercio y Servicios se espera que perciban un salario 23.4% menor que aquellas que están en la categoría de referencia, la que incluye la rama Construcción y otras no especificadas en la base. Ya observábamos indicios de esto en el Cuadro 2, donde la media para esas dos ramas eran visiblemente las más bajas.

La ocupación profesional se distingue del resto (servicios, administrativos y otras no especificadas), siendo el salario esperado para los que pertenecen a esa ocupación un 19.5% mayor que para las demás. Nuevamente, esto es coherente con lo observado en el Cuadro 3. 


# Conclusiones 

Se llegó a un modelo lineal bayesiano válido, ya que al ser evaluado su estimación por MCMC no muestra indicios de falta de convergencia y a su vez muestra una buena capacidad de replicar los datos del logaritmo del salario originales, si bien el modelo presenta limitaciones. Entre estas se encuentra que no reproduce bien el comportamiento de la mediana del logaritmo del salario y tampoco precisamente la forma de la densidad de este último en sus máximos locales. No obstante sí reproduce bien el comportamiento de la media y el rango intercuartílico. 

Encontramos que la estimación de los parámetros mediante el enfoque bayesiano daba resultados similares a los obtenido con el método de los mínimos cuadrados ordinarios.

Para mejorar el modelo se podría profundizar en el uso de la familia de distribuciones Gamma para la variable explicada, con función de enlace logarítmica, así como en la interpretación de los parámetros bajo este contexto.  








\newpage 
# Bibliografía 

* A First Course in Bayesian Statistical Methods.Hoff, Peter. D. Springer. (2009).

* Documentación del paquete _rstanarm_. [Link](http://mc-stan.org/rstanarm/articles/priors.html).

* Introducción a la econometría: Un enfoque moderno. Wooldrige, Jeffrey. Cengage Learning. (2009).

* Materiales del curso 2020 de Modelos Lineales. Nalbarte, Laura. (2020).

* Notas del curso Inferencia II. Alvarez, Ignacio. (2020).



