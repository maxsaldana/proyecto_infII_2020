---
title: "Proyecto Inferencia II"
author: "Emanuelle Marsella, Maximiliano Saldaña, Lucia Tafernaberry"
date: "Noviembre 2020"
output:
  pdf_document:
    toc: no
    pandoc_args: [
      "--number-sections",
      "--number-offset=1"
    ]
header-includes:
  - \usepackage{float}
  - \usepackage[spanish]{babel}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,
                      include=FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.pos = 'H',
                      fig.align = 'center',
                      out.extra = '',
                      fig.hold = 'hold',
                      out.width = "50%"
                      )
options(xtable.comment=FALSE,
        xtable.table.placement="H")
```


```{r, include = FALSE}
library(dplyr)
library(readr)
library(forcats)
library(ggplot2)
library(janitor)

library(xtable)
library(scales)

library(rstanarm)
library(bayesplot)



```

\title{Modelización lineal del salario} 
 \author{Emanuelle Marsella, Maximiliano Saldaña, Lucia Tafernaberry} 
 \date {Noviembre 2020} 



\maketitle

<!-- \vspace{5cm} -->
<!-- ![](unnamed.jpg){width = 50%} -->

\newpage

\tableofcontents

\newpage

# Introducción

En este proyecto aplicaremos los conceptos y métodos bayesianos aprendidos en el curso Inferencia II para analizar los datos seleccionados, una base de datos sobre salarios del año 1976 en Estados Unidos extraída del libro _Introducción a la Econometría, Un enfoque moderno (2009)_ de J.M. Wooldridge. Nuestra intención es tratar de explicar la variable principal, el salario por hora medido en dólares, a partir de las otras variables de la base. Para lograr esto haremos uso de modelos de regresión lineal y metodologías bayesianas asociadas, que implementaremos mediante el software R.   


# Descripción de los datos

La base cuenta con observaciones de 526 personas y con las siguientes 22 variables: Salario (promedio por hora, medido en dólares), Años de Educación, Años de Experiencia, Antigüedad, Raza (variable indicadora, 1 si la persona no es de raza blanca), Sexo, Estado Civil (vale 1 si la persona está casada), Número de Dependientes, Región Metropolitana (vale 1 si la persona vive en dicha región), Región (dividida en tres variales indicadoras: Norte, Sur y Oeste), Rama de Actividad (dividida en tres variables indicadoras: Construcción, Comercio y Servicios), Ocupación (tres variables indicadoras: Profesional, Administrativos y Servicios), el logaritmo de la variable Salario y los cuadrados de las variables Experiencia y Antigüedad.

```{r, results=FALSE, echo=FALSE}
datos <- read_delim("wage1.txt", delim = "\t") %>% 
  as.data.frame %>% 
  clean_names() %>%
  rename(salario = "wage", reg.metro = "smsa", log.salario= "lwage")

as.numeric(sapply(datos, is.numeric))

names(subset(datos, select=!as.numeric(sapply(datos, is.numeric))))

sapply(subset(datos, select=!as.numeric(sapply(datos, is.numeric))), class)

head(subset(datos, select=!as.numeric(sapply(datos, is.numeric))))

datos <- datos %>%
  mutate(salario = gsub(",", ".", salario), log.salario = gsub(",", ".", log.salario) ) %>% 
  mutate_at(c("salario", "log.salario"), as.numeric)
```


## Variables Indicadoras
Inicialmente modificamos la base para llevarla a una forma que nos parecía más práctica de trabajar, agrupando variables que refieren a una sola característica de los datos y estaban en una forma binaria. Estas variables son la Región, separada en la base original en Norte, Sur y Oeste; la Rama de Actividad, separada en Construcción, Servicios y Comercio; y la Ocupación, separada en Profesional, Administrativos y Servicios. Luego de consultar a las docentes de Modelos Lineales optamos por no descartar las variables indicadoras ya que nos permiten su utilización a la hora de aplicar un modelo lineal multivariado y a su vez mantenemos la variable agrupada para poder usarla a la hora de manipular y resumir datos.

```{r, results=FALSE, echo=FALSE}
#mod datos 
datos <- datos %>% 
  mutate(
    region = as.factor(case_when(
               northcen == 1 ~ "northcen",
               south == 1 ~ "south",
               west == 1 ~ "west",
               northcen + south + west == 0 ~ "east"
               )),
    rama_act = as.factor(case_when(
                 construc == 1 ~ "construc",
                 trade == 1 ~ "trade",
                 services == 1 ~ "services",
                 construc + trade + services == 0 ~ "otros"
               )),
    ocupacion = as.factor(case_when(
                 profocc == 1 ~ "profocc",
                 clerocc == 1 ~ "clerocc",
                 servocc == 1 ~ "servocc",
                 profocc + clerocc + servocc == 0 ~ "otros"
                )),
    educsq = educ^2
  ) %>% 
  dplyr::select(-profserv)

#%>% 
 # select(-c(northcen,south, west,construc,trade,services,profserv,profocc,clerocc,servocc))

```

# Exploracion inicial

Para familiarizarnos con la base utilizamos medidas de resumen. 

## Variables categóricas

Lo primero que hacemos es visualizar cómo se distribuye la población de acuerdo a las variables categóricas de las que disponemos (Región, Rama de actividad,  Ocupación, Raza, Sexo y Estado civil). En cuanto a las primeras cuatro variables buscamos saber cuántos individuos pertenecen a cada rama, así como aplicar algunas medidas de resumen de la variable salario para cada categoría de las tres variables, como se puede ver en los cuadros a continuación.


```{r tablasregion, results='asis'}
resumen_tot1 <- datos %>%
                 summarise(region = "Todas",
                           Cantidad=n(),
                           `Mínimo`= min(salario),
                           Media=mean(salario),
                           `Máximo` = max(salario)) %>% 
                rename(`Región` = "region")

resumen_tot2 <- datos %>%
                 summarise(rama_act = "Todas",
                           Cantidad=n(),
                           `Mínimo`= min(salario),
                           Media=mean(salario),
                           `Máximo` = max(salario)) %>% 
                 rename(`Rama de Actividad` = "rama_act")

resumen_tot3 <- datos %>%
                 summarise(ocupacion = "Todas",
                           Cantidad=n(),
                           `Mínimo`= min(salario),
                           Media=mean(salario),
                           `Máximo` = max(salario)) %>%
                 rename(`Ocupación` = "ocupacion")
                 
resumen_reg <- datos %>%
                group_by(region) %>%
                summarise(Cantidad=n(),
                          `Mínimo`= min(salario),
                          Media=mean(salario),
                          `Máximo` = max(salario)) %>%
                arrange(fct_relevel(region,
                                    "northcen",
                                    "south",
                                    "east",
                                    "west")) %>% 
                mutate(region = fct_recode(region,
                                           "Norte"="northcen",
                                           "Sur"="south",
                                           "Este"="east",
                                           "Oeste"="west")) %>% 
                rename(`Región`="region")
  
resumen_rama_act <- datos %>% 
                     group_by(rama_act) %>% 
                     summarise(Cantidad=n(),
                               `Mínimo`= min(salario),
                               Media=mean(salario),
                               `Máximo` = max(salario)) %>%
                     arrange(fct_relevel(rama_act,
                                         "construc",
                                         "services",
                                         "trade",
                                         "otros")) %>% 
                     mutate(rama_act = fct_recode(rama_act,
                                                  "Construcción"="construc",
                                                  "Servicios"="services",
                                                  "Comercio"="trade",
                                                  "Otros"="otros")) %>% 
                     rename(`Rama de Actividad` = "rama_act") 

resumen_ocupacion <- datos %>% 
                      group_by(ocupacion) %>% 
                      summarise(Cantidad=n(),
                                `Mínimo`= min(salario),
                                Media=mean(salario),
                                `Máximo` = max(salario)) %>%
                      arrange(fct_relevel(ocupacion,
                                          "servocc",
                                          "clerocc",
                                          "profocc",
                                          "otros")) %>% 
                      mutate(ocupacion = fct_recode(ocupacion,
                                                   "Servicio"="servocc",
                                                   "Administrativos"="clerocc",
                                                   "Profesional"="profocc",
                                                   "Otros"="otros")) %>%
                      rename(`Ocupación` = "ocupacion")
                     


  


propfem <- table(as.factor(datos$female))/dim(datos)[1]
propwhite <- table(as.factor(datos$nonwhite))/dim(datos)[1]
propmarr <- table(as.factor(datos$married))/dim(datos)[1]
propmetro <- table(as.factor(datos$reg.metro))/dim(datos)[1]
```

### Región
La variable región nos indica en qué región se ubican los individuos. Las categorías de esta variable son Norte, Sur, Este y Oeste.

Tenemos 3 variables indicadoras para esta región, Norte-Centro, Sur y Oeste. Este es la categoría de referencia, la cual no cuenta con variable indicadora ya que en este caso la matriz de datos $\mathbb{X}$ no sería de rango completo y por lo tanto no sería invertible, lo que más adelante impediría la estimación única de los coeficientes. Esto ocurre análogamente para las otras variables cualitativas.

```{r muestrotablasregion, results='asis', include=TRUE}
rbind(resumen_reg, resumen_tot1) %>% 
       xtable(caption = "Algunas medidas de resumen para las distintas regiones.")
```

En primer lugar, podemos observar que la región Oeste cuenta con un número considerablemente pequeño de observaciones y la región sur cuenta con muchas observaciones, en comparación con las demás.

En cuanto al salario por hora, vemos que las medias para las regiones Norte y Sur son menores a la media del total de observaciones, mientras que para las regiones Este y Oeste son mayores. Por otro lado, los máximos son similares, si bien apreciamos que el máximo total se observa en la región este.

Los mínimos también son similares para todas las regiones excepto para la región Oeste, para la cual podemos apreciar que el mínimo es casi 3 veces menor que para las demás.

Como parte de la exploración inicial, también realizamos algunos gráficos de dispersión del salario en función de algunas variables cuantitativas. En ellos podemos apreciar que no hay demasiada diferencia entre las regiones a la hora de explicar el salario a partir de las distintas variables explicativas de forma individual, por lo cual optamos por no incluirlos en este documento.


### Rama de actividad
```{r muestrotablasramas_act, results='asis', include=TRUE}
 rbind(resumen_rama_act, resumen_tot2) %>% 
       xtable(caption = "Algunas medidas de resumen para las distintas ramas de actividad.")
```

Lo primero que observamos, es que la cantidad de observaciones para las distintas categorías varía mucho. En particular, para la categoría "Otros", que es la de referencia, tenemos un gran número de individuos, más de la mitad. Consideramos que esto se debe a que la diferenciación de la que disponemos para las diferentes ramas de actividad no es exhaustiva (como sí sucedía para las regiones, que solo son 4). Por lo tanto, es lógico que suceda que muchos individuos no pertenezcan ni al sector de Construcción, ni al de Servicios, ni al de Comercio; sino que pertenecen a alguna otra categoría que no está especificada y queda incluída dentro de "Otros".

Comparando las regiones en media, observamos que la media de la categoría de referencia es mayor que en las demás ramas de actividad. La categoría Servicios tiene la menor media, y en esta categoría se observa el mínimo total, y el menor de los máximos. La categoría Comercio es similar en media a Servicios, si bien tanto su mínimo como su máximo son mayores. Finalmente, la categoría Construcción tiene la mayor media de las 3 indicadoras de las que disponemos, y también el menor mínimo.

### Ocupación
```{r muestrotablasocupacion, results='asis', include=TRUE}
rbind(resumen_ocupacion, resumen_tot3) %>% 
       xtable(caption = "Algunas medidas de resumen para las distintas ocupaciones.")
```

Podemos observar que para la variable Ocupación, la media de la categoría Profesional es considerablemente mayor respecto de las demás, así como también su máximo. Es razonable esperar que así sea, que las personas que se desempeñan en un empleo profesional perciban un mayor salario, si bien no tenemos fundamentos para afirmar que esta diferencia sea estadísticamente significativa, dado que aún no hemos realizado inferencia a partir de nuestros datos. 

Por otro lado, la categoría Administrativos tiene el mayor mínimo y el menor máximo, y una media menor que la de la categoría de referencia, lo que sugiere que los niveles de salario no varían demasiado, y en general son menores que los de la categoría de referencia.

Luego, la categoría Servicio tiene el menor mínimo, la menor media, y el menor máximo, lo que sugiere que los individuos que se encuentran en esta categoría perciben en promedio un menor salario.


### Otras variables categóricas

Con el resto de las variables econtramos que las proporciones entre las categorías son:

* Para la variable Sexo hay un `r percent(propfem[2], accuracy = 0.01)` de mujeres y  `r percent(propfem[1], accuracy = 0.01)` de hombres. La proporción de la mitad cada sexo.

* Para la variable Estado Civil hay un `r percent(propmarr[2], accuracy = 0.01)` de casados y  `r percent(propmarr[1], accuracy = 0.01)` de no casados.

* Para la variable Área Metropolitana hay un `r percent(propmetro[2], accuracy = 0.01)` de personas que habitan en la misma y  `r percent(propmetro[1], accuracy = 0.01)` que no. Predomina bastante la población metropolitana.

* Para la variable Raza hay un `r percent(propwhite[2],accuracy = 0.01)` de personas no blancas  y  `r percent(propwhite[1], accuracy = 0.01)` blancas. El resultado es de esperarse dado que las personas no blancas son una minoría en los Estados Unidos.

# Métodos

## Presentación del modelo


Vamos a plantear un modelo de la forma
$$log(Y) =  X\beta + \varepsilon$$
donde hacemos los supuestos clásicos sobre los errores: normalidad, esperanza nula, homocedasticidad e incorrelación entre los mismos.

Nuestra variable explicada es el logaritmo del salario, que se puede interpretar de la siguiente forma: dado un aumento unitario en una de las variables explicativas $x_i$, la variación porcentual del salario equivale aproximadamente a $100 \beta_i$, a niveles constantes de todas las demás variables [^1], es decir:

[^1]: Introducción a la econometría: Un enfoque moderno. Wooldrige, Jeffrey. (2009). pág. 43.

$$\% \Delta y \simeq (100 \beta_i) \Delta x$$
$X$ es nuestra matriz de datos, que contiene los valores de las siguientes variables:
Experiencia, Antigüedad, Sexo, Reg. metropolitana, Norte/Sur, Comercio/servicios, Ocup. profesional, Exper. al cuadrado, Educ. al cuadrado, Indic. Obs. 128, Indic. Obs. 381 e Indic. Obs. 440.

Hubo un trabajo previo en el curso de Modelos Lineales para llegar a esta forma del modelo. La selección de las variables se hizo en base al criterio de información de Akaike y contrastes de significación conjunta, que sirvieron para agrupar las variables Norte con Sur y Comercio con Servicios. Además, para que se cumplieran los supuestos se tuvo que realizar una transformación de Box-Cox, excluirse una observación influyente de la base e incluirse un conjunto de variables indicadoras para representar un posible cambio en media y/o varianza en varias observaciones. Estas últimas variables indicadoras son de la forma:

$$z_t = \left\{
    \begin{array}{cc}
         & 1 \; \text{si} \; t=i     \\
         & 0 \; \text{si} \; t\neq i
    \end{array}
\right. $$


Lo que nos dejó con un modelo de la forma.

 $$log(y_t) =  x_t'\beta + \Delta_iz_t + \varepsilon_t$$


## Estimación del modelo 

La estimación de los parámetros del modelo se realizará desde un enfoque bayesiano, es decir, se les asignará una distribución previa para cuantificar la incertidumbre que se tiene sobre sus valores y utilizando los datos se obtendrá una distribución posterior. Se diferencia del caso de un modelo lineal clásico dado que en este último se trabaja con la distribución en el muestreo de lo estimadores de los parámetros y no con la distribución de los parámetros en si.

Realizamos la estimación mediante simulaciones Monte Carlo basadas en cadenas de Markov (MCMC), simulaciones en las cuales cada resultado depende únicamente del anterior. Para la estimación de cada uno de los parámetros se plantean 4 cadenas iniciadas en valores distintos, cada una con 4000 iteraciones. A partir de estos valores se avanza iterativamente hasta  llegar a los 4000 valores simulados, de los cuales los primeros 2000 se usan como "calentamiento" para desligar las simulaciones de los valores iniciales. 

Se desea que las cadenas converjan a la distribución posterior de los parámetros, y aunque nunca podemos asegurar que se da al ser un concepto teórico podemos evaluar si hay indicios de que no se da. Dos métodos para evaluar la convergencia  son el $\hat{R}$ y el $\hat{n}_{eff}$. 

 $$\hat{R} = \sqrt{\frac{Var^+(\theta | y)}{W}} $$
 
Donde:

$$Var^+(\theta | y) = \frac{n-1}{n}W + \frac{1}{n} B $$

En la expresión anterior n indica el número de iteraciones en cada cadena, W la media de las distintas varianzas dentro de las cadenas y B la varianza entre las medias de las cadenas. Si el $\hat{R}$ se acerca a uno, eso significa que la variabilidad entre las cadenas es pequeña y por lo tanto la variabilidad de las simulaciones es explicada por la variabilidad dentro de las cadenas.

Por otro lado definimos el indicador:

$$n_{\text{eff}} = \frac{n.m}{1 + 2 \sum^{\infty}_{k = 1} \rho_k}$$
Donde $n$ es el número de iteraciones, $m$ es el número de cadenas y $\rho_k$ la autocorrelación $k-ésima$. Al estimar $\sum^{\infty}_{k = 1} \rho_k$ conseguimos $\hat{n}_{\text{eff}}$. Este indicador es una estimación de la cantidad de muestras independientes equivalentes a las del método MCMC. 


# Resultados

```{r datosmod, results=FALSE, echo=FALSE }
datos2 <- dplyr::select(datos, -c(region, rama_act, ocupacion)) 



dummy2 <- c(rep(0, 1, 526))
dummy2[128] <- 1
datos2$i128 <- dummy2

dummy4 <- c(rep(0, 1, 526))
dummy4[381] <- 1
datos2$i381 <- dummy4



dummy3 <- c(rep(0, 1, 526))
dummy3[440] <- 1
datos2$i440 <- dummy3

datos2 <- datos2[-24,]

datos2 <- datos2 %>% 
  mutate(
    norsur = ifelse(northcen == 1 | south == 1, 1, 0), 
    comserv = ifelse(trade == 1 | services == 1, 1, 0)
    )

```


Retomando, el modelo a estimar es:

$$log(salario) = \beta_0 + \beta_1exper + \beta_2tenure + \beta_3female + \beta_4reg.metro + \beta_5norsur + \beta_6comserv + \beta_7profocc$$
$$+ \beta_8expersq + \beta_9educsq  + \beta_{10} z_{128} + \beta_{11} z_{381}+ \beta_{12} z_{440} + u_i$$

Inicialmente mantenemos la familia de distribuciones normales para la distribución de los errores que viene por defecto en la función _stan_glm()_ y consecuentemente las previas de los parámetros son definidas como $\beta_0 \sim Normal(1,6 ; 2,5)$ y $\beta_j \sim Normal(0 ; 2,5) \,\, \forall j = 1,...,12$. La media de $\beta_0$ es la media muestral del logaritmo de los salarios, nuestra variable dependiente. A partir de eso, los valores de los desvíos son ajustados de la siguiente forma:

$$\beta_0 \sim Normal(1,6; 2,5\cdot s_y)$$

$$\beta_j \sim Normal(0; 2,5\cdot s_y/s_{x_j}) \,\, \forall j = 1, \dots ,12$$

Donde $s_y$ es el desvío del logaritmo del salario y $s_{x_j}$ es el desvío de la j-ésima variable explicativa. [^1]

En este modelo inicial encontramos el problema de que al comparar las y replicadas con las originales el ajuste no era bueno debido a que los datos simulados eran simétricos a diferencia de los datos originales. Atribuimos esto a que elegimos una distribución Normal para los errores y por lo tanto también para los datos, por lo que optamos por cambiar la familia de distribuciones de los errores por la de las distribuciones Gamma, que permite captar la asimetría de la variable explicada. Las previas luego del ajuste pasan a ser ahora:

$$\beta_0 \sim Normal(0; 2,5)$$

$$\beta_j \sim Normal(0; 2,5/s_{x_j}) \,\, \forall j = 1, \dots ,12$$


```{r, echo=FALSE}
mod_bayes0 <- stan_glm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440,
                      data = datos2,
                      family = gaussian(link = "identity"),
                      seed = 12345,
                      iter = 4000) 



mod_bayes <- stan_glm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440,
                      data = datos2,
                      family = Gamma(link = "log"), #ver que es esto
                      seed = 12345,
                      iter = 4000)

# mod_bayes2 <- stan_glm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440,
#                       data = datos2,
#                       family = Gamma(link = "identity"),
#                       seed = 12345,
#                       iter = 4000)

```

```{r, include = TRUE, fig.cap="Gráficos de las distribuciones de los valores simulados para las dos familias de errores utilizadas", fig.asp=0.25, out.width="200%"}
gridExtra::grid.arrange(
 pp_check(mod_bayes0, nrep = 100) + ggtitle("Familia Normal"),
 pp_check(mod_bayes, nrep = 100) + ggtitle("Familia Gamma"),
 ncol = 2
)
```

Como comentamos anteriormente, podemos observar que las densidades de las $y_{rep}$ ajustan mejor a la distribución original utilizando la familia de densidades Gamma para los errores, conservando mejor la asimetría de la variable dependiente. Cabe destacar que en el caso de la familia Gamma como los valores del recorrido son positivos no se están captando aquellos valores del logaritmo del salario que resultan negativos. Aún así son pocos valores (correspondientes a salarios muy bajos) y para el resto del recorrido el modelo ajusta bien. Por otro lado como una variable aleatoria Gamma tiene recorrido en todos los reales positivos, se le asigna probabilidad positiva a valores del logaritmo del salario que no aparecen en los datos originales, por este motivo las colas derechas del conjunto de distribuciones es más larga.   


[^2]: [Documentación del paquete _rstanarm_.](http://mc-stan.org/rstanarm/articles/priors.html)



```{r modelo bayes y clasico, include=FALSE}
mod_clasico <- lm(log.salario ~ exper + tenure + female + reg.metro + norsur + comserv + profocc + expersq + educsq + i128 + i381 + i440, data=datos2)

parametros.clasicos <- summary(mod_clasico)$coefficients[,1]

summary.mod <- as.data.frame(mod_bayes$stanfit)

parametros.bayesianos <- mod_bayes$coefficients

intervalos.posteriores <- posterior_interval(mod_bayes, prob=0.95)

notacion.numerica <- function(x) {format(x, scientific=FALSE)}
redondear.6 <- function(x) {(round(as.numeric(x), 6))}
```


## Análisis de convergencia

En primer lugar, nos valdremos de distintos indicadores y visualizaciones que nos permitirán disgnósticar si existe o no falta de convergencia en nuestro modelo.

```{r graficos cadenas, include=TRUE, fig.cap="Gráficos que muestran la evolución de las cadenas para dos variables explicativas, el intercepto y la experiencia.", fig.asp=0.25, out.width="200%"}
gridExtra::grid.arrange(
  mcmc_trace(mod_bayes,pars="(Intercept)"),
  mcmc_trace(mod_bayes,pars="exper"),
  ncol=2
  )
```

Para observar el comportamiento de las cadenas usadas en la estimación, comenzamos con gráficos que nos permiten observar la evolución de las mismas a lo largo de las iteraciones. En los mismos podemos ver que las cadenas para los parámetros del intercepto y de la experiencia se mantienen en torno a un cierto valor en cada caso, lo cual sugiere que no hay falta de convergencia. El comportamiento para los demás parámetros es similar, presentándose estos dos casos en el documento de forma de que sirvan de visualización.


Además de las visualizaciones, nos valemos de dos medidas que nos permiten identificar la falta de convergencia o indicios de lo contrario: el $\hat R$ y el $\hat{n}_{\text{eff}}$.

```{r rhat, include=FALSE}
rhat <- summary(mod_bayes) %>% as.data.frame() %>% select(Rhat)
neff <- summary(mod_bayes) %>% as.data.frame() %>% select(n_eff)

print.xtable(
  cbind("Parámetro"=c("Intercepto", 'Experiencia', 'Antigüedad', 'Sexo', 'Región Metropolitana',
                      'Norte/Sur', 'Comercio/Servicios', 'Ocupación Profesional', 'Exp. al cuadrado',
                      'Educ. al cuadrado', 'Indic. 128', 'Indic. 381', 'Indic. 440'),
        "Rhat"=redondear.6(rhat[1:13,]),
        "neff" = redondear.6(neff[1:13,])
        ) %>% xtable(caption="Indicadores sobre convergencia para los distintos parámetros del modelo bayesiano."),
  include.rownames=FALSE)
```

\begin{table}[H]
\centering
\begin{tabular}{lll}
  \hline
Parámetro & $\hat{R}$ & $n_{eff}$ \\ 
  \hline
Intercepto & 0.999805 & 8785 \\ 
  Experiencia & 1.000467 & 5599 \\ 
  Antigüedad & 0.999946 & 9747 \\ 
  Sexo & 0.999805 & 11364 \\ 
  Región Metropolitana & 0.999719 & 11072 \\ 
  Norte/Sur & 0.999737 & 10995 \\ 
  Comercio/Servicios & 0.999618 & 11611 \\ 
  Ocupación Profesional & 0.999887 & 8090 \\ 
  Exp. al cuadrado & 1.000645 & 5719 \\ 
  Educ. al cuadrado & 0.999694 & 8323 \\ 
  Indic. 128 & 0.999767 & 10964 \\ 
  Indic. 381 & 1.00014 & 10004 \\ 
  Indic. 440 & 0.999772 & 11525 \\ 
   \hline
\end{tabular}
\caption{Indicadores sobre convergencia para los distintos parámetros del modelo bayesiano.} 
\end{table}


En este cuadro, podemos ver que para todos los parámetos del modelo bayesiano (del intercepto y las variables explicativas) el valor del $\hat{R}$ está en torno a 1 pero sin superar 1.1, lo cual sirve como regla empírica para desestimar indicios de falta de convergencia. En cuanto al $n_\text{eff}$, los valores son altos en comparación con las 4000 iteraciones dependientes realizadas por cadena, entre 5000 y 12000 simulaciones independientes.

## Distribuciones posteriores y medidas de resumen

```{r cuadro parametros, include=TRUE, results='asis', message=FALSE}
print.xtable(cbind("Parámetro"=c("Intercepto", 'Experiencia', 'Antigüedad', 'Sexo', 'Región Metropolitana', 'Norte/Sur', 'Comercio/Servicios', 'Ocupación Profesional', 'Exp. al cuadrado', 'Educ. al cuadrado', 'Indic. 128', 'Indic. 381', 'Indic. 440'),
      "Parámetros clásicos"= parametros.clasicos,
      "Parámetros bayesianos"=parametros.bayesianos[1:13],
      "Cuantil 2.5%"=round(intervalos.posteriores[1:13,1], 4),
      "Cuantil 97.5%"=round(intervalos.posteriores[1:13,2], 4)
      ) %>%
  as.data.frame() %>%
  mutate_at(vars(2,3), redondear.6) %>%
  xtable(digits = 6,
         caption="Cuadro comparativo entre las estimaciones de los parámetros realizados a través de inferencia clásica y bayesiana."), include.rownames = FALSE)

```

En el cuadro anterior podemos observar los resultados de la estimación de los coeficientes de las variables explicativas, tanto del modelo clásico por medio de mínimos cuadrados ordinarios, como del modelo bayesiano a través de estimación MCMC. Podemos ver que los parámetros estimados se diferencia entre los métodos, que lo atribuímos al cambio de la función link utilizada en el planteo del modelo. Además, planteando para cada parámetro el intervalo de credibilidad al 95%, y en la mayoría los casos vemos que los parámetros estimados del modelo clásico no están comprendidos en dicho intervalo. 

A continuación se presenta una representación gráfica que se corresponde con la información que se presenta en la tabla anterior, en donde los puntos representan la media de la distribución de cada parámetro y la linea el intervalo de credibilidad al 95%.

```{r, include=TRUE, fig.cap="Intervalos de credibilidad al 95\\% y media de la distribución cada parámetro.", fig.asp=0.25, out.width="200%"}
gridExtra::grid.arrange(
  mcmc_intervals(mod_bayes, pars = names(parametros.bayesianos[1:10]), point_est = "mean", prob = 0,
  prob_outer = 0.95) + coord_flip() + theme(axis.text.x = element_text(angle = 45, vjust = 0.75)) ,
  mcmc_intervals(mod_bayes, pars = names(parametros.bayesianos[11:13]), point_est = "mean", prob =0,
  prob_outer = 0.95) + coord_flip() + theme(axis.text.x = element_text(angle = 45)),
  ncol = 2
)
```


## Réplicas de los datos simulados

A continuación, veremos la capacidad predictiva de nuestro modelo, a través de la comparación de estadísticos aplicados tanto a nuestros datos originales como a una muestra de datos simulados.


```{r comparacion datos originales simulados, include=TRUE, fig.cap="Comparación de la media entre los datos originales y los simulados.", fig.asp=0.25, out.width="200%"}
pred<-posterior_predict(mod_bayes,draws = 100)

gridExtra::grid.arrange(pp_check(mod_bayes0, plotfun = "stat", stat = "mean") +  ggtitle("Familia Normal"),
                        pp_check(mod_bayes, plotfun = "stat", stat = "mean") +  ggtitle("Familia Gamma"),
                        ncol=2) 
```

En la visualización anterior, se presentan dos histogramas del logaritmo del salario, en base a 100 simulaciones realizadas con los modelos lineales bayesianos en cada caso. Tanto para el modelo que utiliza la dsitribución Normal de los datos como para el que utiliza la Gamma podemos ver que la media de los datos originales, representada con una línea azul oscura, se ubica sobre los valores centrales del histograma, donde se dan los intervalos con mayor frecuencia relativa. Esto quiere decir que para ambos casos nuestros datos simulados son similares en media a nuestros datos originales. 

```{r, include = TRUE, fig.cap="Comparación de la mediana entre los datos originales y los simulados.", fig.asp=0.25, out.width="200%" }
gridExtra::grid.arrange(pp_check(mod_bayes0, plotfun = "stat", stat = "median"),
                        pp_check(mod_bayes, plotfun = "stat", stat = "median"),
                        ncol=2)
```

No ocurre lo mismo con la mediana, que comentábamos al principio que fue el motivo por el cual optamos por el modelo con distribución de los datos Gamma. Se puede apreciar que para la distribución Normal de los datos el modelo no logra simular el valor de la mediana adecuadamente a diferencia del que utiliza la distribución Gamma.

```{r, include = TRUE, fig.cap="Comparación del rango intercuartílico  entre los datos originales y los simulados.", fig.asp=0.25, out.width="200%" }
gridExtra::grid.arrange(pp_check(mod_bayes0, plotfun = "stat", stat = "IQR"),
                        pp_check(mod_bayes, plotfun = "stat", stat = "IQR"),
                        ncol=2)
```

Ahora tomando como estadístico de referencia el rango intercuartílico, observamos que el modelo inicial lo reproduce levemente mejor. De todas formas, como la diferencia no resulta importante, optamos por el modelo que replica mejor la mediana.

# Conclusión 








\newpage 
# Bibliografía 

* A First Course in Bayesian Statistical Methods.Hoff, Peter. D. Springer. (2009).

* Documentación del paquete _rstanarm_. [Link](http://mc-stan.org/rstanarm/articles/priors.html).

* Introducción a la econometría: Un enfoque moderno. Wooldrige, Jeffrey. Cengage Learning. (2009).

* Materiales del curso 2020 de Modelos Lineales. Nalbarte, Laura. (2020).

* Notas del curso Inferencia II. Alvarez, Ignacio. (2020).



